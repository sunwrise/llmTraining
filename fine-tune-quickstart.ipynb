{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调训练入门\n",
    "\n",
    "本示例将介绍基于 Transformers 实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1e12-1921-4438-8d5d-9760a629dcfe",
   "metadata": {},
   "source": [
    "## YelpReviewFull 数据集\n",
    "\n",
    "**Hugging Face 数据集：[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### 数据集摘要\n",
    "\n",
    "Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。\n",
    "\n",
    "### 支持的任务和排行榜\n",
    "文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。\n",
    "\n",
    "### 语言\n",
    "这些评论主要以英语编写。\n",
    "\n",
    "### 数据集结构\n",
    "\n",
    "#### 数据实例\n",
    "一个典型的数据点包括文本和相应的标签。\n",
    "\n",
    "来自YelpReviewFull测试集的示例如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### 数据字段\n",
    "\n",
    "- 'text': 评论文本使用双引号（\"）转义，任何内部双引号都通过2个双引号（\"\"）转义。换行符使用反斜杠后跟一个 \"n\" 字符转义，即 \"\\n\"。\n",
    "- 'label': 对应于评论的分数（介于1和5之间）。\n",
    "\n",
    "#### 数据拆分\n",
    "\n",
    "Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "\n",
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd17b9a6-166c-4682-8a00-767e5380f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# os.environ['HF_HOME'] = '/autodl-tmp/new_volume/hf' #在transformers自定义模型下载的路径方法\n",
    "# os.environ['HF_HUB_CACHE'] = '/autodl-tmp/new_volume/hf/hub'\n",
    "\n",
    "# os.environ['HF_HOME'] = '/mnt/new_volume/hf' \n",
    "# os.environ['HF_HUB_CACHE'] = '/mnt/new_volume/hf/hub'\n",
    "\n",
    "#在transformers自定义模型下载的路径方法\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/autodl-tmp/datasets_cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"/autodl-tmp/cache/\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/autodl-tmp/hub_cache/\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/autodl-tmp/transform_cache/\"\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c268d1ee-1b3d-44a6-8e09-4834a86775da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http_proxy http://172.20.0.113:12798\n",
      "https_proxy http://172.20.0.113:12798\n",
      "HF_HOME /autodl-tmp/cache/\n",
      "HF_DATASETS_CACHE /autodl-tmp/datasets_cache/\n",
      "HUGGINGFACE_HUB_CACHE /autodl-tmp/hub_cache/\n",
      "TRANSFORMERS_CACHE /autodl-tmp/transform_cache/\n"
     ]
    }
   ],
   "source": [
    "# 验证环境变量是否修改成功\n",
    "print(\"http_proxy\",os.environ.get(\"http_proxy\"))\n",
    "print(\"https_proxy\",os.environ.get(\"https_proxy\"))\n",
    "print(\"HF_HOME\",os.environ.get(\"HF_HOME\"))\n",
    "print(\"HF_DATASETS_CACHE\",os.environ.get(\"HF_DATASETS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE\",os.environ.get(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "print(\"TRANSFORMERS_CACHE\",os.environ.get(\"TRANSFORMERS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6fc806-1395-42dd-8121-a6e98a95cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c94ad529-1604-48bd-8c8d-aa2f3bca6200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\\\n\\\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc45997-e391-456f-b0b9-d3193b0f6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2ecebb-d5d1-456d-967c-842a79fdd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=10):\n",
    "    #这行代码是一个断言语句，它在检查变量 num_examples 是否小于或等于数据集 dataset 的长度。如果这个条件不成立，就会触发一个错误\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "        \n",
    "    picks = []\n",
    "    # 在这段代码中，下划线 _ 是一个通用的占位符，表示我们在这里并不关心循环迭代的具体值。在这里，你使用了for _ in range(num_examples)，表示你打算执行num_examples次循环，但是在循环体内并不需要用到迭代变量的值。\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)   \n",
    "    \n",
    "    #df 就是一个包含了从 dataset 中选取的特定索引数据的 Pandas DataFrame。你可以利用 Pandas 提供的功能对这个 DataFrame 进行各种数据操作和分析。\n",
    "    df = pd.DataFrame(dataset[picks]) \n",
    "    \n",
    "    for column, typ in dataset.features.items():\n",
    "        #通过 isinstance 函数检查当前特征的类型是否是 datasets.ClassLabel，即类别标签\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            # 如果特征是类别标签，就使用 transform 方法将该列的索引值映射为相应的类别标签的名称。这是通过使用 lambda 函数实现的，其中 i 是索引值，typ.names[i] 给出了对应索引值的类别标签的名称。\n",
    "            # tranform方法期待接收一个函数，当你使用 transform 时，你提供的函数将被应用于 DataFrame 或 Series 中的每个元素。这个函数可以是一个已有的函数，也可以是匿名函数（使用 lambda 定义），或者是用户自定义的函数。这个函数的目的是描述如何从一个元素的值转换到另一个值。\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af560b6-7d21-499e-9b82-114be371a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Considering that Gourmet magazine said it was the best Thai restaurant in North America, AND that Frommers highly recommended it, I expected a lot more. My boyfriend and I both had the wonton soup (excellent) and the chicken with cashew nuts. Personally, I've had much better at other Thai restaurants. \\n\\nIt's a run-down little place, but obviously plenty of people know about it. Autographed pictures with celebrities adorn the walls. Service is okay, but nothing exceptional. It was often hard to track down our server.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>I have been a bit absent from Yelp as I lost faith with some hidden reviews but I choose one of my faves in Phoenix to return with a review. True Food has added a wonderful roasted veggie salad to their menu with a horseradish dressing that is bursting with flavor! Their shrimp curry bowl is very good but lacks spice. For a beverage, you must try the Medicine Man. It's an antioxidant pomegranate/black tea based elixir that tastes like a vacation.! The only reason I don't give 5 stars is the dismissive server attitudes and sub-par service we tend to receive. Overall, I highly recommend!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 star</td>\n",
       "      <td>Ugh... Not good.\\n\\nWeekday Night. Restaurant was empty. Four of us(2 adults/2 kids) and five workers(3 cooks) for dinner.\\n\\nMenu is way too complicated for the quality of food. chinese, thai, japanese, bbq?. Wish they stuck to one thing and make it good.\\n\\nAll food delivered separately. First kids meals Lo Mein w/ Teriyaki chicken($4.95).  Looked like one of those weight watchers teriyaki meal. barely filled their plate with noodles. Everything on the plate for the kids looks unappetizing. Brown Noodles and brown chicken. Chicken was tough, very little teriyaki taste.  One of the kids liked, the other not soo much.\\n\\nNext(5 minutes after kids meals came) Pad Thai\\nNothing special. corn flakes on top? bland.\\n\\nNext (5 minutes after Pad Thai) \\nShrimp LoMein - I didn't taste, but the shrimp looked like the small cheap ones that you get buy one - get one at the grocery. Small, unimpressive. Not even medium in size. Throw in at least one large one. How about one medium? Wife said, \\\"Nothing wrong, nothing special\\\"\\n\\nBut, overall odd the time it took to get the food out. With 3 cooks, you think they could have coordinated better. Doesn't shrimp take less time than chicken to cook. I'm thinking the kids chicken may have not been cooked to order. Reheated?\\n\\nSpent $31.  Health Inspection Score on wall was 90.5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 star</td>\n",
       "      <td>I don't know what others are talking about....\\n\\nCons:\\n- Bed was hard\\n- Room layout was awkward \\n- Towels we're rough.... Couldn't tell which was the floor towel and which ones we're body towels\\n- Extra blankets were dirty, like thank goodness I didn't have one of those CSI blue lights, kind of stains....\\n\\nPros:\\n- Staff was friendly\\n- Free breakfast\\n- Walking distance to North Strip casinos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Bachi has a breakfast restaurant?! What!! Obsessed with Bachi Burger as it is, and super disappointed by their ramen, I'm glad HLK lives up to the Bachi name. \\n\\nMy friends and I are fatties and had the Bella farms foie gras de canard (torchon, jelly, marcona almonds, cornichons, cranberry walnut bread) as a starter. Delicious! It has a pretty thick layer of duck fat on top as well. Yeah, as disgusting as you may feel thinking about eating straight up fat, who cares? Spread that stuff on! \\n\\nI had the wild mushroom hash (potato, kale, gruyere, egg, fried onions, bechamel) as well as the corned beef hash (potato, brussel sprouts, onion, egg, bechamel) . No, not all too myself, I'm not that much of a fatty. Disappointed a bit, because none of us were asked how we wanted our eggs cooked. On both hashes they came over hard, which I hate. I love runny yolks, don't take that away from me! Especially on a hash. But other than that, they were both delicious. Corned beef had good flavor. \\n\\nFriend got the braised short ribs loco modo ( two eggs, fried rice, onion rings, orange lentil gravy). Looked bomb. Meat was tender. Braised for thirty six hours. \\n\\nAlso tried the crispy sweet chili chicken and waffle (apple bacon, brussel sprouts, green onion waffle, curry butter). Chicken was very moist, and the sauce on it was delicious. Unfortunately they only put it on one of the pieces. Waffle was ok. Butter ess definitely the high point of the waffle. You get three choices of syrups (maple, coconut, blueberry). \\n\\nOverall, good experience. Great service. Thanks Adrianna! Definitely a come back spot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>The owner was very talkative, but I wouldn't call him rude. He was actually quite nice. I didn't get any attitude for changing my cheesesteak to no onions with provolone instead of cheese wiz. It was delicious too. I wish they had this place in Chicago.\\n\\nVisited: 11/19/09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>I've been dreading this review I'm about to give..  Looking back on the experience, I probably shouldn't go to Japanese restaurants to be turned on, or blown away. My climax with sushi was 3-4 years ago, now I get bored with it. I'm more of a NEW American/French food lover. \\n  First off, the server wasn't very informative, so I talked to another server passing by, she actually gave me some good guidance. We ordered a wide range of items, going off their recommendations. We started with sashimi yellowtail with jalape\\u00f1os. Of course it was good, Yuzu makes anything rock, and fresh raw fish does it's job well, no help needed. Second was the lobster ceviche. I found it boring, uninspired, bland. Sorry.   Third was the new style sashimi, salmon. This was by far the best. Slightly cooked, warm sesame, miso sauce. Delish.  Main courses were the black cod miso,a restaurant favorite, but not my favorite. I just kinda ate it, without having any real feelings for it. I was just thinking ok, maybe the next plate will be better. Creamy spicy rock shrimp tempura, I had two pieces and decided it was time to order a freakin sushi roll and call it quits. I'm hungry and not so impressed. Thank god I wasn't paying.  I ordered the nobu roll I believe, it had ahi tuna, yellowtail, avocado, scallion, masago.  I didn't think I'd have to get a roll to be happy and satisfied, oh well..\\n\\n\\n\\n My only recommendation,  the new style sashimi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>best coffee around.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Too pricey! I couldn't taste ANY alcohol in my wife's birthday cake shake, it tasted good though. The burger a \\\"bigbun\\\" wasn't so big at all! My wife got kogi beef quesadilla. She thought it was awesome! It looked very good, she also got pizza twinkies. I didn't quite get the concept.  I've had waaay better gourmet burgers @ other chains. Fries weren't very good &amp; they didn't give you very many.  There's no shortage on spuds last time I checked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Hands down, the best date place in central Phoenix. \\n\\nSadly, I was apparently not on a date and ended up having to pay for our bill BUT in retrospect I was still so damn impressed by this place.\\n\\nRumor has it that this place is owned by several male flight attendants and when they're not attending flights they are running this restaurant. Leave it to world travelers to produce a classy but quaint but sosticated but understated establishment that can truly please everyone. In true Phoenix fashion, Coronado Cafe was once a house and now stands as a Bistro Cafe. The front porch is now patio seating with cute little tables and bench style sitting all under canopy drapes and string lights. On the inside is the cutest little bar area, very small, but the older gentlemen running the place is so cute and you can tell he really know is cocktails and puts passion into crafting each one. There's also a secluded side patio that's ADORABLE.\\n\\nWe ate inside the dining room because the rain (rain, I know... in Phoenix?) started coming down and the wait staff was so attentive and helped us relocate. We were sat in the corner by a window that had its own little garden bed outside. Could not get over this palces cute factor. Tables were small but candle lit and had the presh checker print table clothes, real napkins and honestly one of the best waitresses I've ever had. \\n\\nWe order 2 Tequilatinis on Happy Hour and the artichoke dip. This artichoke dip opened the gates of heaven and brought to us the most delightful, out of the ordinary, festive taste I have ever had. It was spicy, but cheesy. It was melty but had good size chunks of artichoke. Simply, cannot go on enough about how good it was.\\n\\nI cannot wait to go back here and have brunch or enjoy the sunset with drinks on the patio. If you're looking for somewhere small and intimate that you'd like to take a significant other or truly impress someone you care about, pelase go here. Everything is perfect for the perfect date.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df7cd0-23cd-458f-b2b5-f025c3b9fe62",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bf2b342-e1dd-4ab6-ad57-28eb2513ae38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProxyError",
     "evalue": "(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:980: The handshake operation timed out')))\"), '(Request ID: 88d5509b-7481-4509-8747-e556ab331747)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:700\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_new_proxy_conn \u001b[38;5;129;01mand\u001b[39;00m http_tunnel_required:\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:996\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._prepare_proxy\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    994\u001b[0m     conn\u001b[38;5;241m.\u001b[39mtls_in_tls_required \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    412\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[38;5;241m=\u001b[39m \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msslsocket_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1341\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: _ssl.c:980: The handshake operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:980: The handshake operation timed out')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-cased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 输出分词器的最大序列长度\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Sequence Length for bert-base-cased (tokenizer): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:737\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    739\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:569\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    568\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 569\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:389\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 389\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1628\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/huggingface_hub/file_download.py:408\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:67\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/requests/adapters.py:559\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _ProxyError):\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mProxyError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-cased/resolve/main/tokenizer_config.json (Caused by ProxyError('Cannot connect to proxy.', TimeoutError('_ssl.c:980: The handshake operation timed out')))\"), '(Request ID: 88d5509b-7481-4509-8747-e556ab331747)')"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 输出分词器的最大序列长度\n",
    "print(f\"Max Sequence Length for bert-base-cased (tokenizer): {tokenizer.model_max_length}\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    #padding=\"max_length\" 的意思是，对于每个样本，将其填充到所有样本中最长序列的长度。具体来说，对于较短的序列，将在其末尾填充特殊的标记（通常是 <pad> 或 0）以达到最大长度。而对于超过最大长度的序列，将被截断至最大长度。\n",
    "    #这样做的目的是为了确保所有输入序列的长度一致，以便能够将它们一起批量处理，这对于在深度学习模型中进行高效的训练是很重要的。\n",
    "    # max_length 通常表示模型能够处理的输入序列的最大长度。如果输入序列的长度超过这个值，就会进行截断或填充。\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "#map 方法用于映射函数到数据集的每个元素。在这里，它将 tokenize_function 应用于数据集中的每个样本。参数 batched=True 表示映射函数将按批次处理数据，这样可以提高处理效率。\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 在自然语言处理（NLP）中，特别是在使用预训练的Transformer模型（例如BERT、GPT等）进行文本处理时，attention_mask 是一个用于指示哪些位置需要被模型\"关注\"（即考虑）的二进制掩码。\n",
    "\n",
    "# 具体来说，对于一个输入文本序列，attention_mask 是一个与输入序列等长的二进制序列。\n",
    "# 在这个序列中，每个位置的值可以是0或1，其中：\n",
    "\n",
    "# 0 表示在模型的注意力机制中该位置被掩盖（masked），即模型在处理这个位置时不会考虑它的信息。\n",
    "# 1 表示在模型的注意力机制中该位置是有效的，模型会考虑这个位置的信息。\n",
    "# 使用 attention_mask 的主要目的是允许模型在处理不定长文本时能够处理变长的输入序列，\n",
    "# 因为Transformer模型要求输入序列的长度是固定的。通过将不需要关注的位置置为0，模型就可以正确处理变长的输入。\n",
    "\n",
    "# 在Hugging Face Transformers库中，attention_mask 通常是作为输入参数传递给模型的。\n",
    "# 例如，对于tokenizer的输出，你会得到一个字典，其中包括input_ids和attention_mask。\n",
    "# 将attention_mask传递给模型，有助于模型正确处理变长的输入序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a415a8-cd15-4a8c-851b-9b4740ef8271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Great food!! Amazing atmosphere !!! And best of all fantastic service. Ask for Kyle he is wonderful!</td>\n",
       "      <td>[101, 2038, 2094, 106, 106, 16035, 6814, 106, 106, 106, 1262, 1436, 1104, 1155, 14820, 1555, 119, 18149, 1111, 7156, 1119, 1110, 7310, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33d153-f729-4f04-972c-a764c1cbbb8b",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a17317d8-3c6a-467f-843d-87491f600db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.shuffle(seed=42)：对训练集进行随机打乱，使用了种子 42 以确保可复现性。打乱的目的是防止模型在训练时受到数据顺序的影响，提高模型的泛化性能。\n",
    "#range(1000) 是一个 Python 内置函数，它生成一个包含从0到999（总共1000个元素）的整数序列。这个序列通常用于循环或索引的迭代。\n",
    "#在这个代码中，.select(range(1000)) 的作用是从之前随机打乱的数据集中选择前1000个样本。这样做是为了创建一个规模较小的数据集，只包含1000个样本，用于更快速地进行模型训练和调试。\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b65d63-2d3a-4a56-bc31-6e88a29e9dec",
   "metadata": {},
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d2af4df-abd4-4a4b-94b6-b0e7375304ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04994dd2d891456780aa7626ed2efa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#AutoModelForSequenceClassification 类：\n",
    "# 用于序列分类任务，比如文本分类。\n",
    "# 该类自动加载与预训练模型相对应的分类头（head），并根据任务需求进行微调。\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44014df-b52c-4c72-9e9f-54424725a473",
   "metadata": {},
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98c01d5c-de72-4ff0-b11d-e07ac5346888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"models/bert-base-cased\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100\n",
    "\n",
    "# output_dir：\n",
    "# 指定训练过程中输出模型和其他文件的目录。\n",
    "# 在这里，模型和其他训练过程中的输出将保存在 {model_dir}/test_trainer 目录中。\n",
    "\n",
    "# logging_dir：\n",
    "# 指定 TensorBoard 日志文件的保存目录。\n",
    "# 在这里，TensorBoard 日志将保存在 {model_dir}/test_trainer/runs 目录中。\n",
    "\n",
    "# logging_steps：\n",
    "# 控制多少步骤记录一次训练信息。\n",
    "# 在这里，每进行100个训练步骤，就会记录一次训练信息。\n",
    "\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100,\n",
    "                                  save_total_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ce03480-3aaa-48ea-a0c6-a177b8d8e34f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased/test_trainer/runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased/test_trainer,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased/test_trainer,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd3365-d359-4ab4-a300-4717590cc240",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 \n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ef138-5bf2-41e5-8c68-df8e11f4e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d406c0-56d0-4a54-9c6c-e126ab7f5254",
   "metadata": {},
   "source": [
    "\n",
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f46d2e59-1ebf-43d2-bc86-6b57a4d24d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred 是一个包含两个元素的元组，第一个元素是模型的预测 logits（对各个类别的分数），第二个元素是标签（ground truth）。\n",
    "    # 这一行代码通过解包将 logits 和标签分别赋值给 logits 和 labels 变量。\n",
    "    logits, labels = eval_pred\n",
    "    # 通过使用 np.argmax 函数，找到每个样本预测 logits 中概率最高的类别，即得到模型的预测结果。\n",
    "    # axis=-1 表示在最后一个维度上执行 argmax，对于分类任务通常是类别维度。\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # 调用外部定义的评估指标计算函数 metric.compute，将预测结果 predictions 和真实标签 labels 传递给它。\n",
    "    # 返回的是一个字典，其中包含了计算得到的各个评估指标的数值。\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 当使用 axis=-1 时，它表示在数组的最后一个维度上进行操作。具体来说，考虑一个包含模型对每个示例的三个类别的logits的2D数组（矩阵）：\n",
    "# import numpy as np\n",
    "\n",
    "# logits = np.array([[0.8, 0.2, 0.1],\n",
    "#                    [0.4, 0.6, 0.9],\n",
    "#                    [0.2, 0.5, 0.7]])\n",
    "\n",
    "# 每行对应一个示例的logits，每列对应一个类别的logits。现在，如果你想找到每个示例中具有最高logit的索引（类别），你会使用 np.argmax：\n",
    "# predictions = np.argmax(logits, axis=-1)\n",
    "# print(predictions)\n",
    "\n",
    "# 输出将是一个包含每个示例中具有最高logit的索引（类别）的数组：\n",
    "# [0, 2, 2]\n",
    "\n",
    "# 让我们解释一下 axis=-1 在这个上下文中是如何工作的：\n",
    "\n",
    "# logits数组的形状是 (3, 3)，其中第一个维度对应示例的数量，第二个维度对应类别的数量。\n",
    "# axis=-1 指定了该操作（在本例中是 np.argmax）应该沿着最后一个维度进行。在2D数组中，最后一个维度是第二个维度。\n",
    "# 因此，对于每一行（示例），np.argmax 沿着列（类别）应用，并选择具有最高logit的索引。\n",
    "# 在上面的示例中，结果表明对于第一个示例，具有索引0的类别具有最高的logit，而对于第二个和第三个示例，具有索引2的类别具有最高的logits。这是在处理分类任务时常见的操作，用于确定每个示例在一个批次中的预测类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feba67-9ca9-4793-9a15-3eaa426df2a1",
   "metadata": {},
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afaaee18-4986-4e39-8ad9-b8d413ab4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6981-e444-4c0f-a7cb-dd7f2ba8df12",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）\n",
    "\n",
    "`kernel version` 版本问题：暂不影响本示例代码运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca1d12ac-89dc-4c30-8282-f859724c0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e0db-1168-4a3c-8b75-bfdcef8c5157",
   "metadata": {},
   "source": [
    "## 使用 nvidia-smi 查看 GPU 使用\n",
    "\n",
    "为了实时查看GPU使用情况，可以使用 `watch` 指令实现轮询：`watch -n 1 nvidia-smi`:\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023\n",
    "\n",
    "Wed Dec 20 14:37:41 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "accfe921-471d-481a-96da-c491cdebad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 01:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>1.477182</td>\n",
       "      <td>0.547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>1.625033</td>\n",
       "      <td>0.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>1.832777</td>\n",
       "      <td>0.578000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.28354778798421226, metrics={'train_runtime': 112.2767, 'train_samples_per_second': 26.72, 'train_steps_per_second': 3.34, 'total_flos': 789354427392000.0, 'train_loss': 0.28354778798421226, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "#batch_size默认为8，epcho默认为3轮，所以总步数为3000，每一批次步数为：3000/8=375\n",
    "# batch_size 是深度学习中一个重要的概念，表示每一次模型更新时，输入到模型中的样本数量。\n",
    "# 在训练深度学习模型时，通常会将大量的数据分成小批次进行处理，每个小批次包含固定数量的样本，这就是 batch_size。\n",
    "\n",
    "# 这是通过命令 nvidia-smi 查看 NVIDIA GPU 状态的输出。让我解释一下这个输出：\n",
    "\n",
    "# Driver Version 和 CUDA Version：\n",
    "\n",
    "# Driver Version 表示安装的 NVIDIA 显卡驱动的版本。\n",
    "# CUDA Version 表示安装的 CUDA（Compute Unified Device Architecture）工具包的版本，它是用于在 NVIDIA GPU 上进行并行计算的平台。\n",
    "# GPU 信息：\n",
    "\n",
    "# GPU Name：显卡的名称。\n",
    "# Persistence-M：显卡的持续性模式，\"On\" 表示持续性模式开启。\n",
    "# Bus-Id：显卡在系统总线上的唯一标识。\n",
    "# Disp.A：是否正在使用显卡进行显示，\"Off\" 表示未在使用。\n",
    "# Fan、Temp、Perf：显卡风扇状态、温度、性能状态。\n",
    "# Pwr:Usage/Cap：电源使用量和电源容量。\n",
    "# Memory-Usage：显存使用情况。\n",
    "# GPU-Util：GPU 利用率，表示 GPU 正在被多大程度上利用。\n",
    "# Compute M.：是否支持 Compute 模式。\n",
    "# Processes：\n",
    "\n",
    "# 列出正在使用 GPU 的进程的相关信息，包括 GPU ID、进程 ID（PID）、进程类型、进程名称以及 GPU Memory Usage。\n",
    "# 在你的输出中，主要关注的是 GPU 的状态信息，比如显存使用情况、GPU 利用率、温度等。这些信息可以帮助你监控 GPU 的工作状态，特别是在进行深度学习任务时，可以了解模型的训练过程中 GPU 的负载情况。\n",
    "\n",
    "# 按下 Ctrl + C 即可退出 watch 命令，回到命令行界面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d581099-37a4-4470-b051-1ada38554089",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffb47eab-1370-491e-8a84-6d5347a350b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 04:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1293487548828125,\n",
       " 'eval_accuracy': 0.48,\n",
       " 'eval_runtime': 1.0121,\n",
       " 'eval_samples_per_second': 98.801,\n",
       " 'eval_steps_per_second': 12.844,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a55686-7c43-4ab8-a5cd-0e77f14c7c52",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad0cbc14-9ef7-450f-a1a3-4f92b6486f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{model_dir}/finetuned-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "badf5868-2847-439d-a73e-42d1cca67b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61828934-01da-4fc3-9e75-8d754c25dfbc",
   "metadata": {},
   "source": [
    "## Homework: 使用完整的 YelpReviewFull 数据集训练，对比看 Acc 最高能到多少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645087ba-17df-4f23-81bd-62dfb090b7fe",
   "metadata": {},
   "source": [
    "### 作业2-1：使用完整的 YelpReviewFull 数据集训练，对比看 Acc 最高能到多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75624042-d8b5-4caa-8eb2-ebdc50ac0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# os.environ['HF_HOME'] = '/autodl-tmp/new_volume/hf' #在transformers自定义模型下载的路径方法\n",
    "# os.environ['HF_HUB_CACHE'] = '/autodl-tmp/new_volume/hf/hub'\n",
    "\n",
    "# os.environ['HF_HOME'] = '/mnt/new_volume/hf' \n",
    "# os.environ['HF_HUB_CACHE'] = '/mnt/new_volume/hf/hub'\n",
    "\n",
    "#在transformers自定义模型下载的路径方法\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/autodl-tmp/datasets_cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"/autodl-tmp/cache/\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/autodl-tmp/hub_cache/\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/autodl-tmp/transform_cache/\"\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d169f7ef-d786-4edb-baa7-356f31dd2b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http_proxy http://172.20.0.113:12798\n",
      "https_proxy http://172.20.0.113:12798\n",
      "HF_HOME /autodl-tmp/cache/\n",
      "HF_DATASETS_CACHE /autodl-tmp/datasets_cache/\n",
      "HUGGINGFACE_HUB_CACHE /autodl-tmp/hub_cache/\n",
      "TRANSFORMERS_CACHE /autodl-tmp/transform_cache/\n"
     ]
    }
   ],
   "source": [
    "# 验证环境变量是否修改成功\n",
    "print(\"http_proxy\",os.environ.get(\"http_proxy\"))\n",
    "print(\"https_proxy\",os.environ.get(\"https_proxy\"))\n",
    "print(\"HF_HOME\",os.environ.get(\"HF_HOME\"))\n",
    "print(\"HF_DATASETS_CACHE\",os.environ.get(\"HF_DATASETS_CACHE\"))\n",
    "print(\"HUGGINGFACE_HUB_CACHE\",os.environ.get(\"HUGGINGFACE_HUB_CACHE\"))\n",
    "print(\"TRANSFORMERS_CACHE\",os.environ.get(\"TRANSFORMERS_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ffa35-65c8-450f-822f-ddeb999b7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二次运行时不用执行\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5a4c9-ee81-4b40-9abb-17df80a891c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#第二次运行时不用执行\n",
    "dataset.save_to_disk('../../autodl-tmp/data/yelp_review_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3ec410-ca54-405a-b3f1-0a34ecd1798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('../../autodl-tmp/data/yelp_review_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06341c4a-5aa7-4bb3-998d-a6183c393e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b124b4c-e102-49ba-bba1-80a48ba3c902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'text': \"Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\\\n\\\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b42504-b50c-49cf-9e0d-aeed7f836a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>There are few places that I return to where I consistently eat as much as I can and then walk outside and dry heave (really the other only places are all-you-can-eat sushi joints).  \\n\\nLoLo's is good, very good, but not great.  The fried chicken is good; the skin is delicious.  The scrambled eggs w/ cheese and onions are amazing, but the waffles are just all right.  The people I go with tell me that the grits are fantastic, however grits don't do it for me.  The amount of butter, cheese, and fried goodness is probably enough to actually kill a small child, and certainly enough to send anyone and everyone into a food coma.  Of course you can get smaller portions than the 'K.K.' (3 pieces of fried chicken, 2 waffles, eggs, and grits), but why?!  They also serve Kool-Aid and sweet tea, but they're pretty much just liquid sugar (try half sweet tea, half regular).  And the \\\"homemade\\\" lemonade is definitely just made from those powder lemonade mixes.  There's usually a line, but I've never had to wait more than 10 minutes, and the service ain't great, but that hasn't really ever bothered me (I'm usually too excited for the chicken &amp; waffles to let something like crappy service bother me).  \\n\\nAll-in-all, it's a great place, especially if you're able to take a siesta afterwards (i.e., close the door to your office and pass out for an hour).  But I can't say it's better than Roscoe's in LA; I mean, come on, Roscoe's puts cinnamon in its waffle batter, so good .....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>I'll stay here again, hands down.  \\n\\nI can't thank Derek and his staff enough for the excellent service we had during our three night stay.  Very welcoming staff. \\n\\nThe shuttle picked us up at the airport and offered to take us to our off site rental car location.  We were allowed to check in early.  Anything we needed Derek made it happen.  \\n\\nThe rooms were clean, comfortable perfect for us to come back and relax after the long days we had.  \\n\\nI just wanted to say that it was a very wonderful experience to stay here.  Derek made the trip less stressful making sure we got to where we needed to go.  Keep up the good work, its rare to find that kind of customer service these days.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>One of the Best establishments in Vegas!  I had no complaints at all. Excellent service, Friendly staff.  Eye Candy, Small Gaming area, purple hue lighting everywhere, gigantic chandeliers.  We were on the 50th plus floor.  Might i mention the elevators are super fast.  Rooms are Immaculate and Hip.  View from our balcony overlooked the Bellagio.  Perfect setting at night during the water show.  I was pretty surprised with the self parking garage too.  Modern Art on walls, Green light Red light indicators for aisle slots making it easy to find a parking spot.  Perfect place to bring the Ladies!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>I chose this place based on the great reviews on Yelp and my experience was exactly as expected. I brought my BMW 535 to them for a custom amp install and it looks and sounds great. Highly recommended.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Even if you aren't crazy about M&amp;M's like me, this was a nice little \\\"free\\\" attraction on the strip.  Has four stories of merchandise - top floor has a race car that you can take your picture with.  They have a free movie, however, it was broken while we were there so no show for us.  = (  \\n\\nPrices were a bit steep, but in line with Vegas standards ($10 for a baby bib, $28 for shirts, etc).  The red M&amp;M was out on the sidewalk for photos.  We thought it was a stuffed animal at first because it didn't move and interact with the kids.  We asked and was told that it was a costumed character.  Not sure if it's just hard to move around in there or they weren't really into their job.  All in all, a nice 20 minute walk through attraction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Both my husband and I have had our iPhones repaired here and they have consistently been professional and quick.  There was no sales pressure and the technician was friendly and responsive to any questions I had.  I'd highly recommend the camelback location and we will be back (hopefully not too soon). :).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>Geat customer service ! Can't beat prices ! Staff is so helpful ! Check them out juice is amazing and I vape daily . Wax, herb ect  they have it all . 180 plus flavors .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1 star</td>\n",
       "      <td>This is the devils playground, dont go! \\nFor real.\\n\\nDont serve $1.50 PBR to the kind of peps that go to Philthy Phils. I saw 2 bar fights (same people, two separte fights,) before the music even started. \\nI love a good dive bar..i am no snob.\\nThis is the devils playground.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Late in posting this one. Stayed at the Elara last July for five night and was very pleased. Got a killer deal on one bedroom suite ( was the same price as most regular rooms in the nicer hotels). Room was great....spectacular view, full size automated projection screen, huge jacuzzi bath tub, clean comfortable furnishings, fully equipped kitchen. Pool was immaculate with cool lounge sofas everywhere. Location is perfect as it is connected to a mall, which leads to the Planet Hollywood casino, which then leads to the strip.......the nice part of the strip I might add. The room was quiet too by the way....( of course I didn't room next to the noisy people who posted below who thought having a party in the hallway at 5am was OK because by Vegas standards that's still early ????? )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1 star</td>\n",
       "      <td>The food is absolute garbage.  Period.  Anyone who says otherwise knows nothing about good food.  Happy hour menu is exceptionally bad.  Nice view, but that's about it.  Don't eat there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    #这行代码是一个断言语句，它在检查变量 num_examples 是否小于或等于数据集 dataset 的长度。如果这个条件不成立，就会触发一个错误\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "        \n",
    "    picks = []\n",
    "    # 在这段代码中，下划线 _ 是一个通用的占位符，表示我们在这里并不关心循环迭代的具体值。在这里，你使用了for _ in range(num_examples)，表示你打算执行num_examples次循环，但是在循环体内并不需要用到迭代变量的值。\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)   \n",
    "    \n",
    "    #df 就是一个包含了从 dataset 中选取的特定索引数据的 Pandas DataFrame。你可以利用 Pandas 提供的功能对这个 DataFrame 进行各种数据操作和分析。\n",
    "    df = pd.DataFrame(dataset[picks]) \n",
    "    \n",
    "    for column, typ in dataset.features.items():\n",
    "        #通过 isinstance 函数检查当前特征的类型是否是 datasets.ClassLabel，即类别标签\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            # 如果特征是类别标签，就使用 transform 方法将该列的索引值映射为相应的类别标签的名称。这是通过使用 lambda 函数实现的，其中 i 是索引值，typ.names[i] 给出了对应索引值的类别标签的名称。\n",
    "            # tranform方法期待接收一个函数，当你使用 transform 时，你提供的函数将被应用于 DataFrame 或 Series 中的每个元素。这个函数可以是一个已有的函数，也可以是匿名函数（使用 lambda 定义），或者是用户自定义的函数。这个函数的目的是描述如何从一个元素的值转换到另一个值。\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55df2c74-11a2-49bb-9e76-bf06d6df0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用自定义代理，一般情况下不使用\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "#在transformers自定义模型下载的路径方法\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/autodl-tmp/datasets_cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"/autodl-tmp/cache/\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/autodl-tmp/hub_cache/\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/autodl-tmp/transform_cache/\"\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/profile.d/clash.sh && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbc16acb-20cf-44fb-a02a-9bc7961cd81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length for bert-base-cased (tokenizer): 512\n"
     ]
    }
   ],
   "source": [
    "#仅在第一次运行时使用\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "config = AutoConfig.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "cache_directory = '../../autodl-tmp/tokenizer/bert-base-cased'\n",
    "\n",
    "# 如果不存在，则新建文件路径\n",
    "if not os.path.exists(cache_directory):\n",
    "    os.makedirs(cache_directory)\n",
    "\n",
    "tokenizer.save_pretrained(cache_directory)\n",
    "config.save_pretrained(cache_directory)\n",
    "\n",
    "print(f\"Max Sequence Length for bert-base-cased (tokenizer): {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671cb3a1-3ad6-496f-891c-ca58b66a201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length for bert-base-cased (tokenizer): 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# from transformers import AutoConfig\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# 指定缓存路径\n",
    "cache_directory = '../../autodl-tmp/tokenizer/bert-base-cased'\n",
    "\n",
    "# 指定模型名称或路径\n",
    "# model_name_or_path = \"bert-base-cased\"\n",
    "\n",
    "# 加载模型配置\n",
    "# config = AutoConfig.from_pretrained(cache_directory)\n",
    "\n",
    "# 输出最大序列长度\n",
    "# print(f\"Max Sequence Length for bert-base-cased: {config.max_position_embeddings}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cache_directory)\n",
    "\n",
    "# 输出分词器的最大序列长度\n",
    "print(f\"Max Sequence Length for bert-base-cased (tokenizer): {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5467cd9-73b6-4fce-8dbe-fa55dd8d80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    #padding=\"max_length\" 的意思是，对于每个样本，将其填充到所有样本中最长序列的长度。具体来说，对于较短的序列，将在其末尾填充特殊的标记（通常是 <pad> 或 0）以达到最大长度。而对于超过最大长度的序列，将被截断至最大长度。\n",
    "    #这样做的目的是为了确保所有输入序列的长度一致，以便能够将它们一起批量处理，这对于在深度学习模型中进行高效的训练是很重要的。\n",
    "    # max_length 通常表示模型能够处理的输入序列的最大长度。如果输入序列的长度超过这个值，就会进行截断或填充。\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_attention_mask=True)\n",
    "\n",
    "#map 方法用于映射函数到数据集的每个元素。在这里，它将 tokenize_function 应用于数据集中的每个样本。参数 batched=True 表示映射函数将按批次处理数据，这样可以提高处理效率。\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 在自然语言处理（NLP）中，特别是在使用预训练的Transformer模型（例如BERT、GPT等）进行文本处理时，attention_mask 是一个用于指示哪些位置需要被模型\"关注\"（即考虑）的二进制掩码。\n",
    "\n",
    "# 具体来说，对于一个输入文本序列，attention_mask 是一个与输入序列等长的二进制序列。\n",
    "# 在这个序列中，每个位置的值可以是0或1，其中：\n",
    "\n",
    "# 0 表示在模型的注意力机制中该位置被掩盖（masked），即模型在处理这个位置时不会考虑它的信息。\n",
    "# 1 表示在模型的注意力机制中该位置是有效的，模型会考虑这个位置的信息。\n",
    "# 使用 attention_mask 的主要目的是允许模型在处理不定长文本时能够处理变长的输入序列，\n",
    "# 因为Transformer模型要求输入序列的长度是固定的。通过将不需要关注的位置置为0，模型就可以正确处理变长的输入。\n",
    "\n",
    "# 在Hugging Face Transformers库中，attention_mask 通常是作为输入参数传递给模型的。\n",
    "# 例如，对于tokenizer的输出，你会得到一个字典，其中包括input_ids和attention_mask。\n",
    "# 将attention_mask传递给模型，有助于模型正确处理变长的输入序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549c3c84-c5f5-4661-8117-2ea335b2eac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length for bert-base-cased: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Liquid was very clean and modern. Small layout. $50 dollar minimum. $15 drinks. However, the pool boys know what customer service means. On hot days, get the frozen fruits $16, its frozen goodness!</td>\n",
       "      <td>[101, 5255, 24235, 1108, 1304, 4044, 1105, 2030, 119, 6844, 9726, 119, 109, 1851, 8876, 5867, 119, 109, 1405, 8898, 119, 1438, 117, 1103, 4528, 3287, 1221, 1184, 8132, 1555, 2086, 119, 1212, 2633, 1552, 117, 1243, 1103, 7958, 11669, 109, 1479, 117, 1157, 7958, 18023, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>There once was a chick who was sure\\nShe would dig this new club at Encore.\\nThough the guest list was free\\nThe dudes were FUG-ly\\nAnd the best part was just the d\\u00e9cor. \\n\\n:)</td>\n",
       "      <td>[101, 1247, 1517, 1108, 170, 22282, 1150, 1108, 1612, 165, 183, 1708, 4638, 1156, 11902, 1142, 1207, 1526, 1120, 13832, 9475, 119, 165, 183, 1942, 14640, 5084, 1103, 3648, 2190, 1108, 1714, 165, 183, 1942, 4638, 17869, 1116, 1127, 143, 2591, 2349, 118, 181, 1183, 165, 183, 1592, 3276, 1103, 1436, 1226, 1108, 1198, 1103, 173, 165, 190, 7629, 1162, 1580, 19248, 119, 165, 183, 165, 183, 131, 114, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 输出分词器的最大序列长度\n",
    "print(f\"Max Sequence Length for bert-base-cased: {tokenizer.model_max_length}\")\n",
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5784cae-8d2d-4f68-b061-145d8e1d8efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# 检查数据是否被处理过\n",
    "for i,example in enumerate(tokenized_datasets[\"train\"]):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(len(example['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb5764d5-57b8-40bc-8ea8-659034cffadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../autodl-tmp/model/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 先使用命令行将模型下载到本地指定文件夹\n",
    "# huggingface-cli download --resume-download --local-dir-use-symlinks False bert-base-cased --local-dir /root/autodl-tmp/model/bert-base-cased\n",
    "\n",
    "#AutoModelForSequenceClassification 类：\n",
    "# 用于序列分类任务，比如文本分类。\n",
    "# 该类自动加载与预训练模型相对应的分类头（head），并根据任务需求进行微调。\n",
    "\n",
    "# 指定缓存路径\n",
    "cache_directory = '../../autodl-tmp/model/bert-base-cased'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(cache_directory, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a137fc5-6653-4db8-ad20-fa91db320146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network connection is successful.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def test_network_connection(url=\"http://huggingface.co\"):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        print(\"Network connection is successful.\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Network connection failed. Error: {e}\")\n",
    "\n",
    "# 测试网络连接\n",
    "test_network_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44ad039e-6309-45eb-97fc-1a30b4d702a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取结果为: EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted labels.\n",
      "    references (`list` of `int`): Ground truth labels.\n",
      "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple example\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.5}\n",
      "\n",
      "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
      "        >>> print(results)\n",
      "        {'accuracy': 3.0}\n",
      "\n",
      "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.8778625954198473}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "#前置已经从GitHub：https://github.com/huggingface/evaluate/tree/main 上面下载了accruracy文件到本地\n",
    "# 因为直接load（accuracy）的话，会联网，没翻墙读取不了\n",
    "\n",
    "file_path = '../../autodl-tmp/evaluate/metrics/accuracy'\n",
    "\n",
    "metric = evaluate.load(file_path)\n",
    "\n",
    "print(f\"读取结果为: {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83a6ed42-cd69-4bbf-92d8-5024c0dd030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # eval_pred 是一个包含两个元素的元组，第一个元素是模型的预测 logits（对各个类别的分数），第二个元素是标签（ground truth）。\n",
    "    # 这一行代码通过解包将 logits 和标签分别赋值给 logits 和 labels 变量。\n",
    "    logits, labels = eval_pred\n",
    "    # 通过使用 np.argmax 函数，找到每个样本预测 logits 中概率最高的类别，即得到模型的预测结果。\n",
    "    # axis=-1 表示在最后一个维度上执行 argmax，对于分类任务通常是类别维度。\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # 调用外部定义的评估指标计算函数 metric.compute，将预测结果 predictions 和真实标签 labels 传递给它。\n",
    "    # 返回的是一个字典，其中包含了计算得到的各个评估指标的数值。\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 当使用 axis=-1 时，它表示在数组的最后一个维度上进行操作。具体来说，考虑一个包含模型对每个示例的三个类别的logits的2D数组（矩阵）：\n",
    "# import numpy as np\n",
    "\n",
    "# logits = np.array([[0.8, 0.2, 0.1],\n",
    "#                    [0.4, 0.6, 0.9],\n",
    "#                    [0.2, 0.5, 0.7]])\n",
    "\n",
    "# 每行对应一个示例的logits，每列对应一个类别的logits。现在，如果你想找到每个示例中具有最高logit的索引（类别），你会使用 np.argmax：\n",
    "# predictions = np.argmax(logits, axis=-1)\n",
    "# print(predictions)\n",
    "\n",
    "# 输出将是一个包含每个示例中具有最高logit的索引（类别）的数组：\n",
    "# [0, 2, 2]\n",
    "\n",
    "# 让我们解释一下 axis=-1 在这个上下文中是如何工作的：\n",
    "\n",
    "# logits数组的形状是 (3, 3)，其中第一个维度对应示例的数量，第二个维度对应类别的数量。\n",
    "# axis=-1 指定了该操作（在本例中是 np.argmax）应该沿着最后一个维度进行。在2D数组中，最后一个维度是第二个维度。\n",
    "# 因此，对于每一行（示例），np.argmax 沿着列（类别）应用，并选择具有最高logit的索引。\n",
    "# 在上面的示例中，结果表明对于第一个示例，具有索引0的类别具有最高的logit，而对于第二个和第三个示例，具有索引2的类别具有最高的logits。这是在处理分类任务时常见的操作，用于确定每个示例在一个批次中的预测类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ef931d8-b227-4e11-ae5b-621254bf6f34",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=../../autodl-tmp/model/bert-base-cased-trained/test_trainer/runs,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1000,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=../../autodl-tmp/model/bert-base-cased-trained/test_trainer,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=31,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=../../autodl-tmp/model/bert-base-cased-trained/test_trainer,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_dir = '../../autodl-tmp/model/bert-base-cased-trained' #\"models/bert-base-cased-all\"\n",
    "batch_size=31\n",
    "\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=1000,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  save_total_limit=5)\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为1000\n",
    "\n",
    "# output_dir：\n",
    "# 指定训练过程中输出模型和其他文件的目录。\n",
    "# 在这里，模型和其他训练过程中的输出将保存在 {model_dir}/test_trainer 目录中。\n",
    "\n",
    "# logging_dir：\n",
    "# 指定 TensorBoard 日志文件的保存目录。\n",
    "# 在这里，TensorBoard 日志将保存在 {model_dir}/test_trainer/runs 目录中。\n",
    "\n",
    "# logging_steps：\n",
    "# 控制多少步骤记录一次训练信息。\n",
    "# 在这里，每进行1000个训练步骤，就会记录一次训练信息。\n",
    "\n",
    "#save_total_limit用于设置checkpoint最多保留几个文件，防止训练过程中断点拍照文件对硬盘占用过多\n",
    "\n",
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2c10650-1f3c-4248-b68a-124c5df53c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9881d89d-e381-4c81-a703-6721a598bb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62904' max='62904' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62904/62904 5:34:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.697371</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.667900</td>\n",
       "      <td>0.672976</td>\n",
       "      <td>0.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.550700</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=62904, training_loss=0.5141013600391755, metrics={'train_runtime': 20072.0677, 'train_samples_per_second': 97.15, 'train_steps_per_second': 3.134, 'total_flos': 5.1361871752428134e+17, 'train_loss': 0.5141013600391755, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n",
    "#batch_size默认为8，epcho默认为3轮，所以总步数为3000，每一批次步数为：3000/8=375\n",
    "# batch_size 是深度学习中一个重要的概念，表示每一次模型更新时，输入到模型中的样本数量。\n",
    "# 在训练深度学习模型时，通常会将大量的数据分成小批次进行处理，每个小批次包含固定数量的样本，这就是 batch_size。\n",
    "\n",
    "# 这是通过命令 nvidia-smi 查看 NVIDIA GPU 状态的输出。让我解释一下这个输出：\n",
    "\n",
    "# Driver Version 和 CUDA Version：\n",
    "\n",
    "# Driver Version 表示安装的 NVIDIA 显卡驱动的版本。\n",
    "# CUDA Version 表示安装的 CUDA（Compute Unified Device Architecture）工具包的版本，它是用于在 NVIDIA GPU 上进行并行计算的平台。\n",
    "# GPU 信息：\n",
    "\n",
    "# GPU Name：显卡的名称。\n",
    "# Persistence-M：显卡的持续性模式，\"On\" 表示持续性模式开启。\n",
    "# Bus-Id：显卡在系统总线上的唯一标识。\n",
    "# Disp.A：是否正在使用显卡进行显示，\"Off\" 表示未在使用。\n",
    "# Fan、Temp、Perf：显卡风扇状态、温度、性能状态。\n",
    "# Pwr:Usage/Cap：电源使用量和电源容量。\n",
    "# Memory-Usage：显存使用情况。\n",
    "# GPU-Util：GPU 利用率，表示 GPU 正在被多大程度上利用。\n",
    "# Compute M.：是否支持 Compute 模式。\n",
    "# Processes：\n",
    "\n",
    "# 列出正在使用 GPU 的进程的相关信息，包括 GPU ID、进程 ID（PID）、进程类型、进程名称以及 GPU Memory Usage。\n",
    "# 在你的输出中，主要关注的是 GPU 的状态信息，比如显存使用情况、GPU 利用率、温度等。这些信息可以帮助你监控 GPU 的工作状态，特别是在进行深度学习任务时，可以了解模型的训练过程中 GPU 的负载情况。\n",
    "\n",
    "# 按下 Ctrl + C 即可退出 watch 命令，回到命令行界面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7baf717f-f071-4ee4-b41e-a15a4b86147e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8521449565887451,\n",
       " 'eval_accuracy': 0.61,\n",
       " 'eval_runtime': 0.5673,\n",
       " 'eval_samples_per_second': 176.283,\n",
       " 'eval_steps_per_second': 22.917,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))\n",
    "\n",
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bcab914-fab0-4d91-85f0-6ea300b8e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"{model_dir}/finetuned-trainer\")\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e7e94-3c59-4a49-82c8-8c1c95131e85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
